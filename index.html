
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Hengran Zhang</title>
</head>
<body>
    <div id="layout-content">

        <table class="imgtable">
            <tr>
                <td>
                    <a href=https://hengran.github.io/"><img src="picture/hengran.jpg" alt="alt text" width="125px" /></a>&nbsp;
                </td>
                <td align="left">
                    <ul>
                        <li>
                            <p align="justify">
                                Hengran Zhang&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.ucas.ac.cn/"><img src="picture/cas.png" width="100px" height="100px" /></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://www.ict.ac.cn/"><img src="picture/ict.jpeg" width="80px" height="80px" /></a><br />
                            </p>
                        </li>
                    </ul>
                    <ul>
                        <li>
                            <p align="justify">
                                <b>Research Assistant</b>,
                                <a href="http://www.ict.ac.cn/">Institute of Computing Technology, Chinese Academy of Sciences</a>
                                <br />
                            </p>
                        </li>
                    </ul>

                    <ul>
                        <li>
                            <p align="justify">
                                Address: No.6, Kexueyuan South Road, Zhongguancun, Haidian District, Beijing, China
                                <br />
                            </p>
                        </li>
                    </ul>
                    <ul>
                        <li>
                            <p align="justify">
                                Email: zhanghengran22z [at] ict.ac.cn<br />
                            </p>
                        </li>
                    </ul>
                    <!--<a href="https://scholar.google.com/citations?user=ysrrJqsAAAAJ&hl=zh-CN">[Google Scholar]</a>-->
                    <!--<a href="https://hengran.github.io/chinese">[中文主页]</a>-->
                </td>
            </tr>
        </table>



   
        <h2><font color="Brown">About Me</font></h2>
        <p align="justify">
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            I'm currently a PHD student and a research assistant with the Institute of Computing Technology, Chinese Academy of Sciences under the supervision of Prof. Jiafeng Guo.
        </p>
        <p align="justify">
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            My research interests include retrieval-augmented method.

        </p>
     


        <h2><font color="Brown">News!!!</font></h2>

        <ul>
	    <li>
                <p align="justify">
                    2023.10: Our paper "Cort: A new baseline for comparative opinion classification by dual prompts" is accepted by EMNLP2022 Findings
                    <br />
                </p>
            </li>
            <li>
                <p align="justify">
                    2023.10: Our paper "From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification" is accepted by EMNLP2023 Findings
                    <br />
                </p>
            </li>
	 <li>
                <p align="justify">
                    2023.10: Our paper "Are Large Language Models Good at Utility Judgments?" is accepted by SIGIR2024 (CCF-A)
                    <br />
                </p>
            </li>
        </ul>


    
       


       
    
        <h2>Selected Publications</h2>
        <h3>Representative Publications</h3>
     
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:0;margin-left:0;font-size:0.95em;">
            <tbody>
                <tr>
                   <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border:none;">
                        <p align="justify"><b>Cort: A new baseline for comparative opinion classification by dual prompts</b> </p>
                        <p align="justify">Yequan Wang, <b>Hengran Zhang</b>,Aixin sun, Xuying Meng </p>
                        <p align="justify">
				Comparative opinion is a common linguistic phenomenon. The opinion is expressed by comparing multiple targets on a shared aspect, e.g., “camera A is better than camera B in picture quality”. Among the various subtasks in opinion mining, comparative opinion classification is relatively less studied. Current solutions use rules or classifiers to identify opinions, i.e., better, worse, or same, through feature engineering. Because the features are directly derived from the input sentence, these solutions are sensitive to the order of the targets mentioned in the sentence. For example, “camera A is better than camera B” means the same as “camera B is worse than camera A”; but the features of these two sentences are completely different. In this paper, we approach comparative opinion classification through prompt learning, taking the advantage of embedded knowledge in pre-trained language model. We design a twin framework with dual prompts, named CORT. This extremely simple model delivers state-of-the-art and robust performance on all benchmark datasets for comparative opinion classification. We believe CORT well serves as a new baseline for comparative opinion classification.
                          </p>

                    </td>
                    <td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border:none;">
                        <p align="justify"><b>From Relevance to Utility: Evidence Retrieval with Feedback for Fact Verification</b> </p>
                        <p align="justify"><b>Hengran Zhang</b>, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Maarten_de_Rijke, Yixing Fan, Xueqi Cheng </p>
                        <p align="justify">
                           Retrieval-enhanced methods have become a primary approach in fact verification (FV); it requires reasoning over multiple retrieved pieces of evidence to verify the integrity of a claim. To retrieve evidence, existing work often employs off-the-shelf retrieval models whose design is based on the probability ranking principle. We argue that, rather than relevance, for FV we need to focus on the utility that a claim verifier derives from the retrieved evidence. We introduce the 
  feedback-based evidence retriever(FER) that optimizes the evidence retrieval process by incorporating feedback from the claim verifier. As a feedback signal we use the divergence in utility between how effectively the verifier utilizes the retrieved evidence and the ground-truth evidence to produce the final claim label. Empirical studies demonstrate the superiority of FER over prevailing baselines.
                        </p>

                    </td>
		<td style="padding:20px;width:70%;vertical-align:middle;border-right:none;border:none;">
                        <p align="justify"><b>Are Large Language Models Good at Utility Judgments?</b> </p>
                        <p align="justify"><b>Hengran Zhang</b>, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Maarten_de_Rijke, Yixing Fan, Xueqi Cheng </p>
                        <p align="justify">
                          Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs.

                    </td>
                </tr>
            </tbody>
        </table>


       




        <h3>Selected Publication List</h3>
        <ul>
            <li>
                <p align="justify">
                    <a href="https://ieeexplore.ieee.org/document/10163770">
                        CORT: A New Baseline for Comparative Opinion Classification by Dual Prompts.
                    </a>
                    <br />
                    Yequan Wang, <b>Hengran Zhang</b>, Aixin Sun, Xuying Meng<br />
                    <i>Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7093 - 7104
December 7-11, 2022 </i> 2023
                </p>
            </li>
        </ul>



       
	
</body>
</html>
